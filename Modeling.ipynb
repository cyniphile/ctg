{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "from mord import LogisticAT\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score\n",
    "from kappa_loss_perceptron import KappaLossPerceptron\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "from skll.metrics import kappa\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(rc={'figure.figsize': (15.7, 8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import is_classifier\n",
    "LogisticAT._estimator_type = \"classifier\" # type: ignore\n",
    "assert is_classifier(LogisticAT) \n",
    "assert is_classifier(KappaLossPerceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- properly collect model outputs from nested kfold loop\n",
    "\t- doesn't matter at this stage\n",
    "- ensure stratification\n",
    "\t- default for gridsearch, if classifier, had to reimpliment for perceptron AND mord\n",
    "\t- ![](2021-12-10-10-31-54.png) \n",
    "- reunderstand classweight\n",
    "- double check categorical/1-hot values are working correctly\n",
    "\n",
    "- MAYBE try custom loss for xgboost IF THERE's time\n",
    "https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some clean data. Now we can thing about modeling, starting with transformations.\n",
    "\n",
    "0) Ensure categorical features are identified and encoded as such\n",
    "\t- All features are continuous with the exception of `Tendency` which is ordinal (and can still be treated as continuous)\n",
    "1) We want to normalize everything that's scalar so that regularization works properly\n",
    "2) We'll want to try out a PCA step since many inputs are correlated\n",
    "3) Because of class imbalance, we'll want to at least think about options like SMOTE\n",
    "### For training:\n",
    "1) We have a smaller dataset, so a nested k-fold will work. The inner fold is for hyperparam optimization, the outer is for evaluating model performance. \n",
    "2) We have imbalanced classes so k-folding should be stratified by class\n",
    "### For modeling, we need to choose classification models. \n",
    "1) we're dealing with an ordinal regression, not just classification. \n",
    "2) We need to think of smart error metrics\n",
    "\n",
    "Goal: Add in learning curve with one model and see how scalable it is to use on a basked\n",
    "\n",
    "Sometimes when you want to debug, you just a get a lot of \"abstract\" data types. I found it's helpful to temporarily disable JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/preprocessed_data.csv\", index_col=0)\n",
    "\n",
    "TARGET = \"NSP\"\n",
    "FEATURES = df.columns.drop(TARGET)\n",
    "\n",
    "# Reserve a \"final final\" test set\n",
    "X = df[FEATURES].values\n",
    "y = df[TARGET].values.astype(int) - 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight between None and \"Balanced\"\n",
    "class_weight_intermediate = {0: 1, 1: 2, 2: 3}\n",
    "class_weight_high = {0: 1, 1: 10, 2: 20}\n",
    "model_params = {\n",
    "    'logisticOVR': {\n",
    "        'model__C': [1, 5, 10],\n",
    "        'model__class_weight': ['balanced', None, class_weight_intermediate, class_weight_high]\n",
    "    },\n",
    "    'logisticMN': {\n",
    "        'model__C': [1, 5, 10],\n",
    "        'model__class_weight': ['balanced', None, class_weight_intermediate, class_weight_high]\n",
    "    },\n",
    "    'kappaPerceptron': {\n",
    "    },\n",
    "    'ordinal': {'model__alpha': [0, 0.5, 1, 2]},\n",
    "    'RF': {\n",
    "        'model__n_estimators': [1, 10, 20],\n",
    "        'model__max_features': ['sqrt', None],\n",
    "        'model__max_depth': [None, 5, 10, 20],\n",
    "        'model__class_weight': ['balanced', None, class_weight_intermediate, class_weight_high]\n",
    "    },\n",
    "    'rbfSVM': {\n",
    "        'model__C': [1, 5, 10],\n",
    "        'model__gamma': [0.001, 0.0001],\n",
    "        'model__class_weight': ['balanced', None, class_weight_intermediate, class_weight_high]\n",
    "    },\n",
    "    'KNN': {'model__n_neighbors': [3, 7, 10],\n",
    "            'model__weights': ['uniform', 'distance']},\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [1, 10, 20],\n",
    "        'model__max_features': ['sqrt', None],\n",
    "        'model__max_depth': [None, 5, 10, 20],\n",
    "        'model__class_weight': ['balanced', None, class_weight_intermediate, class_weight_high]\n",
    "    }\n",
    "}\n",
    "\n",
    "pipe_params = {\n",
    "    'PCA': [None, PCA(), PCA(15), PCA(10), PCA(5)],\n",
    "    'smote': [None, SMOTE()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error metrics for each class of fetal state\n",
    "f1Scorer1 = make_scorer(lambda x, y: f1_score(x, y, average=None)[0])\n",
    "f1Scorer2 = make_scorer(lambda x, y: f1_score(x, y, average=None)[1])\n",
    "f1Scorer3 = make_scorer(lambda x, y: f1_score(x, y, average=None)[2])\n",
    "precScorer1 = make_scorer(lambda x, y: precision_score(x, y, average=None)[0])\n",
    "precScorer2 = make_scorer(lambda x, y: precision_score(x, y, average=None)[1])\n",
    "precScorer3 = make_scorer(lambda x, y: precision_score(x, y, average=None)[2])\n",
    "recallScorer1 = make_scorer(lambda x, y: recall_score(x, y, average=None)[0])\n",
    "recallScorer2 = make_scorer(lambda x, y: recall_score(x, y, average=None)[1])\n",
    "recallScorer3 = make_scorer(lambda x, y: recall_score(x, y, average=None)[2])\n",
    "\n",
    "# Penalties for different types of class confusion\n",
    "weights = np.array([\n",
    "# Predicted   N    S    P     # True\n",
    "            [0.0, 0.5, 0.6],  # N\n",
    "            [1.0, 0.0, 0.3],  # S\n",
    "            [2.0, 0.6, 0.0]   # P\n",
    "])\n",
    "def weightedKappa(x, y): return kappa(x, y, weights=weights)\n",
    "\n",
    "kappaScorer = make_scorer(weightedKappa)\n",
    "\n",
    "complete_scorer = {\n",
    "    'f1-N': f1Scorer1, 'f1-S': f1Scorer2, 'f1-P': f1Scorer3,\n",
    "    'prec-N': precScorer1, 'prec-S': precScorer2, 'prec-P': precScorer3,\n",
    "    'recall-N': recallScorer1, 'recall-S': recallScorer2, 'recall-P': recallScorer3,\n",
    "    'weighted-kappa': kappaScorer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(model):\n",
    "    return Pipeline(steps=[\n",
    "               ('smote', SMOTE()),\n",
    "               ('scaler', StandardScaler()),\n",
    "               ('PCA', PCA()),\n",
    "               ('model', model)\n",
    "           ])\n",
    "\n",
    "def make_inner_kfold(pipeline, preprocessing_grid, model_grid):\n",
    "    full_grid = {}\n",
    "    full_grid.update(preprocessing_grid)\n",
    "    full_grid.update(model_grid)\n",
    "    return GridSearchCV(pipeline, param_grid=full_grid, cv=2, n_jobs=-1, \n",
    "                        scoring=kappaScorer)\n",
    "\n",
    "models = {\n",
    "    \"logisticOVR\": LogisticRegression(max_iter=5000, multi_class='ovr'),\n",
    "    \"logisticMN\": LogisticRegression(\n",
    "        max_iter=5000, multi_class='multinomial'\n",
    "    ),\n",
    "    \"kappaPerceptron\": KappaLossPerceptron(\n",
    "        num_classes=3, weight_matrix=weights, max_iter=5000\n",
    "    ),\n",
    "    \"ordinal\": LogisticAT(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"rbfSVM\": svm.SVC(kernel=\"rbf\"),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "piped_models = {\n",
    "    key: make_pipeline(model) \n",
    "    for key, model in models.items()\n",
    "}\n",
    "\n",
    "piped_folded_models = {\n",
    "    key: make_inner_kfold(pipe, pipe_params, model_params[key])\n",
    "    for key, pipe in piped_models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('smote', SMOTE()), ('scaler', StandardScaler()),\n",
       "                ('PCA', PCA()),\n",
       "                ('model',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               eval_metric='mlogloss', gamma=0, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=12, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=None, subsample=1,\n",
       "                               tree_method='exact', use_label_encoder=False,\n",
       "                               validate_parameters=1, ...))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piped_models['XGBoost'].fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', class_weight=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=20,\n",
       "              max_features='sqrt', min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=20, n_jobs=12,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1, ...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piped_folded_models['XGBoost'].best_estimator_['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 203 iterations.\n",
      "Stopping early after 211 iterations.\n",
      "Stopping early after 24 iterations.\n",
      "Stopping early after 20 iterations.\n",
      "Stopping early after 199 iterations.\n",
      "Stopping early after 23 iterations.\n",
      "Stopping early after 28 iterations.\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 11 iterations.\n",
      "Stopping early after 143 iterations.\n",
      "Stopping early after 198 iterations.\n",
      "Stopping early after 143 iterations.\n",
      "Stopping early after 21 iterations.\n",
      "Stopping early after 24 iterations.\n",
      "Stopping early after 25 iterations.\n",
      "Stopping early after 173 iterations.\n",
      "Stopping early after 208 iterations.\n",
      "Stopping early after 130 iterations.\n",
      "Stopping early after 238 iterations.\n",
      "Stopping early after 209 iterations.\n",
      "Stopping early after 140 iterations.\n",
      "Stopping early after 146 iterations.\n",
      "Stopping early after 220 iterations.\n",
      "Stopping early after 162 iterations.\n",
      "Stopping early after 207 iterations.\n",
      "Stopping early after 140 iterations.\n",
      "Stopping early after 207 iterations.\n",
      "Stopping early after 220 iterations.\n",
      "Stopping early after 221 iterations.\n",
      "Stopping early after 207 iterations.\n",
      "Stopping early after 177 iterations.\n",
      "Stopping early after 134 iterations.\n",
      "Stopping early after 217 iterations.\n",
      "Stopping early after 198 iterations.\n",
      "Stopping early after 116 iterations.\n",
      "Stopping early after 242 iterations.\n",
      "Stopping early after 214 iterations.\n",
      "Stopping early after 168 iterations.\n",
      "Stopping early after 65 iterations.\n",
      "Stopping early after 179 iterations.\n",
      "Stopping early after 248 iterations.\n",
      "Stopping early after 208 iterations.\n",
      "Stopping early after 176 iterations.Stopping early after 116 iterations.\n",
      "\n",
      "Stopping early after 200 iterations.\n",
      "Stopping early after 217 iterations.\n",
      "Stopping early after 201 iterations.\n",
      "Stopping early after 234 iterations.\n",
      "Stopping early after 212 iterations.\n",
      "Stopping early after 201 iterations.\n",
      "Stopping early after 211 iterations.\n",
      "Stopping early after 167 iterations.\n",
      "Stopping early after 231 iterations.\n",
      "Stopping early after 221 iterations.\n",
      "Stopping early after 245 iterations.\n",
      "Stopping early after 161 iterations.\n",
      "Stopping early after 164 iterations.\n",
      "Stopping early after 163 iterations.\n",
      "Stopping early after 216 iterations.\n",
      "Stopping early after 199 iterations.\n",
      "Stopping early after 185 iterations.\n",
      "Stopping early after 208 iterations.\n",
      "Stopping early after 157 iterations.\n",
      "Stopping early after 221 iterations.\n",
      "Stopping early after 156 iterations.\n",
      "Stopping early after 149 iterations.\n",
      "Stopping early after 189 iterations.\n",
      "Stopping early after 176 iterations.\n",
      "Stopping early after 113 iterations.\n",
      "Stopping early after 231 iterations.\n",
      "Stopping early after 87 iterations.\n",
      "Stopping early after 174 iterations.\n",
      "Stopping early after 251 iterations.\n",
      "Stopping early after 147 iterations.\n",
      "Stopping early after 133 iterations.\n",
      "Stopping early after 96 iterations.\n",
      "Stopping early after 208 iterations.\n",
      "Stopping early after 140 iterations.\n",
      "Stopping early after 180 iterations.\n",
      "Stopping early after 148 iterations.\n",
      "Stopping early after 164 iterations.\n",
      "Stopping early after 139 iterations.\n",
      "Stopping early after 161 iterations.\n",
      "Stopping early after 174 iterations.\n",
      "Stopping early after 144 iterations.\n",
      "Stopping early after 226 iterations.\n",
      "Stopping early after 174 iterations.\n",
      "Stopping early after 156 iterations.\n",
      "Stopping early after 152 iterations.\n",
      "Stopping early after 339 iterations.\n",
      "Stopping early after 133 iterations.\n",
      "Stopping early after 198 iterations.\n",
      "Stopping early after 141 iterations.\n",
      "Stopping early after 206 iterations.\n",
      "[CV] END  f1-N: (test=0.849) f1-P: (test=0.522) f1-S: (test=0.524) prec-N: (test=0.964) prec-P: (test=0.429) prec-S: (test=0.402) recall-N: (test=0.758) recall-P: (test=0.667) recall-S: (test=0.750) weighted-kappa: (test=0.596) total time=  21.9s\n",
      "Stopping early after 224 iterations.\n",
      "Stopping early after 158 iterations.\n",
      "Stopping early after 351 iterations.\n",
      "[CV] END  f1-N: (test=0.888) f1-P: (test=0.630) f1-S: (test=0.593) prec-N: (test=0.976) prec-P: (test=0.489) prec-S: (test=0.500) recall-N: (test=0.815) recall-P: (test=0.885) recall-S: (test=0.727) weighted-kappa: (test=0.707) total time=  22.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   22.2s remaining:   33.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early after 208 iterations.\n",
      "[CV] END  f1-N: (test=0.894) f1-P: (test=0.586) f1-S: (test=0.598) prec-N: (test=0.967) prec-P: (test=0.531) prec-S: (test=0.479) recall-N: (test=0.831) recall-P: (test=0.654) recall-S: (test=0.795) weighted-kappa: (test=0.671) total time=  22.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   22.6s remaining:   15.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early after 239 iterations.\n",
      "[CV] END  f1-N: (test=0.891) f1-P: (test=0.714) f1-S: (test=0.595) prec-N: (test=0.990) prec-P: (test=0.667) prec-S: (test=0.453) recall-N: (test=0.810) recall-P: (test=0.769) recall-S: (test=0.867) weighted-kappa: (test=0.730) total time=  23.0s\n",
      "Stopping early after 228 iterations.\n",
      "[CV] END  f1-N: (test=0.879) f1-P: (test=0.656) f1-S: (test=0.587) prec-N: (test=0.985) prec-P: (test=0.568) prec-S: (test=0.451) recall-N: (test=0.794) recall-P: (test=0.778) recall-S: (test=0.841) weighted-kappa: (test=0.703) total time=  23.4s\n",
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   23.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   23.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n",
      "[CV] START .....................................................................\n",
      "[CV] END  f1-N: (test=0.899) f1-P: (test=0.780) f1-S: (test=0.537) prec-N: (test=0.986) prec-P: (test=0.719) prec-S: (test=0.418) recall-N: (test=0.827) recall-P: (test=0.852) recall-S: (test=0.750) weighted-kappa: (test=0.744) total time=   8.0s\n",
      "[CV] END  f1-N: (test=0.882) f1-P: (test=0.806) f1-S: (test=0.475) prec-N: (test=0.967) prec-P: (test=0.694) prec-S: (test=0.378) recall-N: (test=0.811) recall-P: (test=0.962) recall-S: (test=0.636) weighted-kappa: (test=0.701) total time=   8.0s\n",
      "[CV] END  f1-N: (test=0.891) f1-P: (test=0.800) f1-S: (test=0.538) prec-N: (test=0.971) prec-P: (test=0.833) prec-S: (test=0.412) recall-N: (test=0.823) recall-P: (test=0.769) recall-S: (test=0.778) weighted-kappa: (test=0.710) total time=   8.0s\n",
      "[CV] END  f1-N: (test=0.924) f1-P: (test=0.667) f1-S: (test=0.554) prec-N: (test=0.965) prec-P: (test=0.727) prec-S: (test=0.456) recall-N: (test=0.887) recall-P: (test=0.615) recall-S: (test=0.705) weighted-kappa: (test=0.725) total time=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    8.1s remaining:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    8.1s remaining:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  f1-N: (test=0.886) f1-P: (test=0.655) f1-S: (test=0.480) prec-N: (test=0.967) prec-P: (test=0.643) prec-S: (test=0.370) recall-N: (test=0.819) recall-P: (test=0.667) recall-S: (test=0.682) weighted-kappa: (test=0.674) total time=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.7s finished\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for key, model in piped_folded_models.items():\n",
    "    scores[key] = cross_validate(\n",
    "        model, X=X_train, y=y_train, \n",
    "        cv=5, scoring=complete_scorer, n_jobs=-1, verbose=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/st/28t68bk15ks5j8v29x3_8x3m0000gp/T/ipykernel_84204/2314000820.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d945a4ff71f07307950fda08398aacc03d9c1d10eb9aad6ef4af91767460bd95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
